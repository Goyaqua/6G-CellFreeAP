# Scenario 7: High Interference + QoS-Focused Reward
# 25 APs, 20 Users - Dense network with performance-focused reward

network:
  num_aps: 25
  num_users: 20                  # HIGH INTERFERENCE
  num_antennas_per_ap: 1
  area_size: 500.0
  carrier_frequency: 3.5e9
  bandwidth: 10e6
  max_power_per_ap: 0.2
  noise_power_dbm: -94
  circuit_power_per_ap: 0.2

environment:
  qos_min_rate_mbps: 5.0
  qos_weight: 100.0              # QoS-FOCUSED REWARD
  episode_length: 100
  action_type: 'discrete'
  num_power_levels: 5
  randomize_circuit_power: true  # CIRCUIT POWER ADAPTIVE
  circuit_power_range: [0.1, 0.5]

training:
  total_timesteps: 10000
  eval_freq: 5000
  save_freq: 20000
  n_eval_episodes: 10

  dqn:
    learning_rate: 0.0001
    buffer_size: 150000
    learning_starts: 3000
    batch_size: 64
    gamma: 0.99
    tau: 1.0
    train_freq: 4
    gradient_steps: 1
    target_update_interval: 1000
    exploration_fraction: 0.3
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.05

evaluation:
  n_episodes: 100
  baseline_strategies:
    - 'nearest_max'
    - 'equal_all'
    - 'load_balance'

output:
  results_dir: './results'
  logs_dir: './logs'
  plots_dir: './plots'
  tensorboard_log: './tensorboard'
  save_models: true
  save_plots: true
  verbose: 1

seed:
  env_seed: 42
  agent_seed: 42
  eval_seed: 123
