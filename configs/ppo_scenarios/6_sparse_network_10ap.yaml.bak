# üõ°Ô∏è Priority 6: SPARSE NETWORK ROBUSTNESS - 10 APs
# 10 APs, 5 Users, Balanced Reward
# Scientific Goal: Test RL robustness in resource-limited scenarios
# Prove: RL adapts to sparse deployments, doesn't overfit to dense networks

network:
  num_aps: 10                    # SPARSE: Limited AP resources
  num_users: 5                   # Proportionally fewer users
  num_antennas_per_ap: 1
  area_size: 500.0
  carrier_frequency: 3.5e9
  bandwidth: 10e6
  max_power_per_ap: 0.2
  noise_power_dbm: -94
  circuit_power_per_ap: 0.2

environment:
  qos_min_rate_mbps: 5.0
  qos_weight: 2.0                # BALANCED: log_EE (~8) vs QoS penalty (~20 max)
  episode_length: 50
  action_type: 'discrete'
  num_power_levels: 5
  randomize_circuit_power: true  # CIRCUIT POWER ADAPTIVE
  circuit_power_range: [0.1, 0.5]

training:
  total_timesteps: 100000        # Smaller action space, fewer steps needed
  eval_freq: 10000
  save_freq: 10000
  n_eval_episodes: 5

  # PPO Configuration
  ppo:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 256       # IMPROVED: More stable gradients
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.03           # IMPROVED: More exploration (fights lazy agent)
    vf_coef: 0.5
    max_grad_norm: 0.5

evaluation:
  n_episodes: 100
  baseline_strategies:
    - 'nearest_max'
    - 'equal_all'
    - 'load_balance'

output:
  results_dir: './results'
  logs_dir: './logs'
  plots_dir: './plots'
  tensorboard_log: './tensorboard'
  save_models: true
  save_plots: true
  verbose: 1

seed:
  env_seed: 42
  agent_seed: 42
  eval_seed: 123
