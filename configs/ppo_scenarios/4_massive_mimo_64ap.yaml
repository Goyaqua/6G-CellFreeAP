# ðŸ¥‰ Priority 4: MASSIVE MIMO GENERALIZATION - 64 APs
# 64 APs, 10 Users, Balanced Reward (can switch to Green if needed)
# Scientific Goal: Show RL can shut down unnecessary APs in massive deployments
# Prove: RL corrects the inefficiency seen in 64 AP baseline graphs

network:
  num_aps: 64                    # MASSIVE: Many APs available
  num_users: 10
  num_antennas_per_ap: 1
  area_size: 500.0
  carrier_frequency: 3.5e9
  bandwidth: 10e6
  max_power_per_ap: 0.2
  noise_power_dbm: -94
  circuit_power_per_ap: 0.2

environment:
  qos_min_rate_mbps: 5.0
  qos_weight: 2.0                # BALANCED: log_EE (~8) vs QoS penalty (~20 max)
  episode_length: 50
  action_type: 'discrete'
  num_power_levels: 5
  randomize_circuit_power: true  # CIRCUIT POWER ADAPTIVE
  circuit_power_range: [0.1, 0.5]

training:
  total_timesteps: 200000        # Many APs = complex action space
  eval_freq: 10000
  save_freq: 20000
  n_eval_episodes: 5

  # PPO Configuration
  ppo:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 256       # IMPROVED: More stable gradients
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.05           # IMPROVED: Even more exploration for 64 APs!
    vf_coef: 0.5
    max_grad_norm: 0.5

evaluation:
  n_episodes: 100
  baseline_strategies:
    - 'nearest_max'
    - 'equal_all'
    - 'load_balance'

output:
  results_dir: './results'
  logs_dir: './logs'
  plots_dir: './plots'
  tensorboard_log: './tensorboard'
  save_models: true
  save_plots: true
  verbose: 1

seed:
  env_seed: 42
  agent_seed: 42
  eval_seed: 123
